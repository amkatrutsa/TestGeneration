\documentclass[a4paper,12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[labelfont=bf,labelsep=space]{caption}
\usepackage{subfigure}
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{amsfonts,amssymb,amsfonts,amsthm,amsmath}
\usepackage[left = 2cm, top = 2cm, right = 2cm, bottom = 2.2cm]{geometry}
%\usepackage{mathtext}
\usepackage{hyperref}
%\usepackage{multirow}
%\usepackage{url}
%\usepackage{lineno}
\usepackage[usenames]{color}
\usepackage{colortbl}
\usepackage[all]{xy}
\usepackage{enumerate}
\usepackage{morefloats}
\usepackage{abstract}
\usepackage{indentfirst}
\usepackage[nottoc,notlof,notlot]{tocbibind}
%\newtheorem{Th}{Теорема}
%\newtheorem{Lem}{Лемма}
%\newtheorem{Def}{Определение}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calJ}{\mathcal{J}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\frakD}{\mathfrak{D}}
\newcommand{\vif}{\mathrm{VIF}}
\newcommand{\rss}{\mathrm{RSS}}
\newcommand{\tss}{\mathrm{TSS}}
\newcommand{\bic}{\mathrm{BIC}}
\newcommand{\radj}{R_{\text{adj}}^2}
\renewcommand{\thesubfigure}{\asbuk{subfigure})}
%\renewcommand*{\thesubfigure}{\alph{subfigure})}
%\renewcommand{\thesubfigure}{\asbuk{subfigure}}
%\renewcommand{\thesection}{\arabic{section}}
%\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\newcommand{\T}{{\text{\tiny\sffamily\upshape\mdseries T}}}
\theoremstyle{plain}
%\parindent = 1cm
\renewcommand{\baselinestretch}{1.3}
\graphicspath{{./fig/ortcol/} {./fig/coltarget/} {./fig/sumort/}}
\makeatletter
\let\@fnsymbol\@arabic
\makeatother
\AtBeginDocument{\renewcommand{\abstractname}{}}
%\renewcommand{\abstractname}{}
\AtBeginDocument{\renewcommand{\absnamepos}{empty}}
\setlength{\parindent}{1cm}
\makeatletter
\renewcommand\@biblabel[1]{#1.}
\makeatother


\begin{document}
\title{ПРОБЛЕМА МУЛЬТИКОЛЛИНЕАРНОСТИ ПРИ ВЫБОРЕ ПРИЗНАКОВ В РЕГРЕССИОННЫХ ЗАДАЧАХ}
\author{А.М. Катруца\thanks{Московский физико-технический институт, amkatrutsa@yandex.ru}, В. В. Стрижов\thanks{Вычислительный центр им. А.А. Дородницына РАН, strijov@gmail.com}}
\date{}
\maketitle
\begin{abstract}

\textbf{Аннотация:} В данной работе исследуется проблема мультиколлинеарности и её влияние на эффективность методов выбора признаков. Предлагается процедура тестирования методов выбора признаков и методика порождения тестовых выборок с различными типами мультиколлинеарности между признаками. Рассматриваемые методы выбора признаков тестируются на порождённых выборках. Процедура тестирования заключается в применении методов выбора признаков к выборкам с различным типом мультиколлинеарности и оценивании количества мультиколлинеарных признаков в множестве отобранных признаков. В работе приводится критерий сравнения методов выбора признаков, на котором основана процедура их тестирования. Также методы выбора признаков сравниваются согласно различным функционалам качества. Проведено сравнение методов выбора признаков в случае наличия в данных определённого типа мультиколлинеарности, и сделан вывод о качестве работы рассматриваемых методов на определённых типах данных.  

\textbf{Ключевые слова:} регрессионный анализ, выбор признаков, мультиколлинеарность, тестовые выборки, критерий качества. 
\end{abstract}

\section{Введение}
 
Работа посвящена тестированию методов выбора признаков. Предполагается, что исследуемая выборка содержит значительное число мультиколлинеарных признаков. \emph{Мультиколлинеарность}~--- это сильная корреляционная связь между отбираемыми для анализа признаками, совместно воздействующими на целевой вектор, которая затрудняет оценивание регрессионных параметров и выявление зависимости между признаками и целевым вектором. Проблема мультиколлинеарности, возможные способы её обнаружения и устранения описаны в~\cite{multRegression,multBayesInterpret,belsley2005regression}. Также мультиколлинеарность приводит к уменьшению устойчивости оценок вектора параметров. Оценка вектора параметров называется устойчивой, если малое изменении некоторой компоненты этого вектора приводит к малому изменению соответствующей компоненты оценки целевого вектора.

В задачах анализа данных для уменьшения размерности~\cite{yu2003feature,strijov2012rankscale}, упрощения использования стандартных алгоритмов машинного обучения~\cite{chen2006combining}, удаления нерелевантных признаков~\cite{john1994irrelevant} и повышения обобщающей способности применяемого алгоритма~\cite{voron2008combprob} применяются методы выбора признаков. Также методы выбора признаков используются для решения проблемы мультиколлинеарности в задачах регрессии~\cite{performChong}. 

Задача выбора оптимального подмножества признаков является одной из основных задач предварительной обработки данных. Методы выбора признаков основаны на минимизации некоторого функционала, который отражает качество рассматриваемого подмножества признаков. В~\cite{introVariableFeatureSelection,
reviewFeatureSelectionOnSyntData,ladha2011feature} сделан обзор существующих методов выбора признаков, проведена классификация методов выбора признаков по используемым функционалам качества и стратегии поиска оптимального подмножества признаков. 
%и способам предъявления нового потенциально лучшего подмножества для.

При наличии мультиколлинеарности в регрессионных задачах применение методов выбора признаков приводит к повышению устойчивости оценок параметров и уменьшению их дисперсии. 
%Следствием наличия мультиколлинеарности в задачах регрессии является неустойчивость оценок параметров и рост их дисперсии.
%Методы выбора признаков решают проблемы неустойчивости оценок коэффициентов, роста их дисперсии. Эти проблемы являются следствием наличия мультиколлинеарных признаков. 
Для этого используются методы отбора признаков с различными регуляризаторами или стратегиями добавления и удаления признаков с использованием статистических тестов для проверки значимости добавляемого признака. Примерами методов, использующих регуляризаторы, являются гребневая регрессия~\cite{ridge}, где регуляризатор~--- взвешенная евклидова норма вектора параметров, Lasso~\cite{lasso} и LARS~\cite{lars}, где регуляризатор~--- взвешенная сумма модулей параметров, Elastic net~\cite{elnet}, где регуляризатор~--- линейная комбинация предыдущих двух регуляризаторов. Методом, использующим проверку значимости добавляемого или удаляемого признака является шаговая регрессия~\cite{stepwise} с различными комбинациями процедур добавления или удаления признаков      

%Для выбора признаков предложены следующие методы: шаговая регрессия~\cite{stepwise}, гребневая регрессия~\cite{ridge}, Lasso~\cite{lasso}, LARS~\cite{lars}, Elastic net~\cite{elnet}. Применение шаговой регрессии основано на переборе некоторых подмножеств признаков с целью поиска тех признаков, которые вносят наибольший вклад в вариацию зависимой переменной. Для добавления признака необходимо, чтобы множество признаков, включающее рассматриваемый признак, удовлетворяло некоторому критерию, например t-тесту или F-тесту. Гребневая регрессия является регуляризованным методов наименьших квадратов, где в качестве регуляризатора выбрана сумма квадратов параметров регрессии. Алгоритм LARS аналогичен алгоритму шаговой регрессии, только вместо добавления признаков, алгоритм изменяет их веса так, чтобы обеспечить наибольшую корреляцию с вектором регрессионных остатков. 
%Метод Lasso аналогичен гребневой регрессии, с точностью до регуляризатора --- в Lasso используется сумма модулей весов параметров регрессии.
%Elastic Net является комбинацией методов LARS и Lasso. В этом методе регуляризация задана в виде взвешенной суммы регляризаторов из LARS и Lasso.   
%
%%Задача выбора признаков в общем случае является задачей дискретного программирования, решаемая  
%В основе каждого метода отбора признаков лежит некоторый функционал качества текущего подмножества признаков и метод добавления/удаления признаков из набора или же процедура настройки весов.      
%
%Из приведённых выше методов отбора признаков проблему мультиколлинеарности решают Elastic net~\cite{elnet}, LASSO~\cite{lasso}, LARS~\cite{lars}, гребевая регрессия~\cite{ridge}. В этих методах используются различные виды регуляризации функционалов качества, что позволяет избегать роста параметров регрессии и получать более устойчивые оценки.   

Для тестирования методов выбора признаков в~\cite{performChong} предложен метод генерации выборок и функционал, позволяющий оценить качество процедуры выбора признаков. Однако предложенный способ не позволяет оценить изменение критерия качества при непрерывном изменении параметров выборок и структурного параметра мультиколлинеарности. 

В нашей работе предложена другая процедура генерации тестовых выборок, основанная на задании свойств признаков. Рассматриваются следующие свойства признаков: мультиколлинеарность между признаками, коррелированность целевому вектору, ортогональность между признаками, ортогональность признаков целевому вектору. Задание количества признаков обладающих каждым из этих свойств позволяет генерировать выборки с различным взаимным расположением признаков и целевого вектора. Такой метод генерации тестовых выборок даёт возможность исследовать зависимость эффективности методов выбора признаков при непрерывном изменении параметра мультиколлинеарности. 

В работе предложен критерий ранжирования методов выбора признаков и методика их тестирования.
%Также предлагается способ тестирования методов выбора признаков и процедура генерации тестовых выборок. 
Критерием ранжирования является количество мультиколлинеарных признаков в множестве отобранных признаков удаление которых приводит к росту ошибки не больше некоторого заданного значения.
Методика тестирования заключается в последовательном применении различных методов выбора признаков к тестовым выборкам, каждая из которых отражает некоторый тип мультиколлинеарности и оценке качества полученного подмножества признаков для каждой пары, включающей метод выбора признаков и тестовую выборку. 

%В \cite{SanduleanuFeatureSel} предложен алгоритм, который решает проблему мультиколлинеарности.\footnote{Не слишком ли претенциозно?} В работе проведено сравнение алгоритма из \cite{SanduleanuFeatureSel} и существующих методов, решающих проблему мултиколлинеарности по различным функционалам качества, описанным в\cite{multCausesEffectsRemedies}.   

\section{Постановка задачи выбора признаков}

Задана выборка $\frakD = \{ (\bx_i, y_i) \}$, $i \in \mathcal{I} = \{1, \ldots, m \} $, множество свободных переменных~--- вектор $\bx = [x_1, \ldots, x_j, \ldots, x_n] $, где $j \in \mathcal{J} = \{ 1, \ldots, n\}$. Предполагается, что эти переменные принадлежат множеству действительных чисел, либо его подмножеству: $\bx_i \in \mathbb{X} \subseteq \mathbb{R}^n$ и $y_i \in \mathbb{Y} \subseteq \mathbb{R}^1$. Введём обозначения: $\by = [y_1, \ldots, y_m]^{\T}$~--- вектор значений зависимой переменной, целевой вектор, $\bchi_j = [x_{1j}, \ldots, x_{mj}]^{\T}$~--- реализация $j$-ой свободной переменной, $j$-ый признак и $\mathbf{X} = [\bx^{\T}_1, \ldots, \bx^{\T}_m]^{\T} = [\bchi_1, \ldots, \bchi_n]$~--- матрица плана. Предполагается, что вектор $\bx_i$ и число $y_i$ связаны соотношением:
\begin{equation}
y_i = f(\bw, \bx_i) + \varepsilon(\bx_i),
\label{eq:regr}
\end{equation}
где $f: \mathbb{W} \times \mathbb{X} \rightarrow \mathbb{Y}$ отображение декартова произведения пространства допустимых параметров $\mathbb{W}$ и пространства значений $\mathbb{X}$ свободной переменной в область значений $\mathbb{Y}$ зависимой переменной, а $\varepsilon(\bx_i)$~--- $i$-ый компонент вектора регрессионных остатков $\boldsymbol{\varepsilon} = \mathbf{f} - \by$. Обозначим вектор-функцию: 
\begin{equation*}
\mathbf{f} = \mathbf{f}(\bw, \mathbf{X}) = [f(\bw, \bx_1), \ldots, f(\bw, \bx_m)]^{\T} \in \mathbb{Y}^m.
\label{eq:model}
\end{equation*}

Определим функцию ошибки 
\begin{equation*}
S: \mathbb{X} \times \mathbb{W} \times \mathbb{Y} \rightarrow \mathbb{R}_+
\end{equation*}
и представление множества индексов элементов выборки в виде: 
\begin{equation*}
\mathcal{I} = \mathcal{L} \cup \mathcal{C}.
\label{eq:learntest}
\end{equation*}
Далее в качестве функции ошибки $S$ зададим квадрат нормы вектора регрессионных остатков $\boldsymbol{\varepsilon}$:
\begin{equation}
S = \sum\limits_{i = 1}^m \varepsilon^2(\bx_i) = \rss, \qquad \tss = \sum\limits_{i = 1}^m (y_i - \overline{y})^2, \; \text{где} \; \overline{y} = \frac{1}{m}\sum\limits_{i = 1}^m y_i.
\label{eq:error}
\end{equation} 
Требуется найти такой оптимальный вектор параметров $\bw^* \in \mathbb{W}$, что функция $f(\bw^*, \bx)$ приближает целевой вектор $\by$ наилучшим образом в смысле функции ошибки $S$. 
%Для этого рассмотрим две вспомогательные задачи.

Назовём моделью пару $(\mathbf{f}, \calA)$, где $\calA \subset \calJ$~--- подмножество индексов признаков, используемое для вычисления вектор-функции $\mathbf{f}$. Ниже фиксирована функция $\mathbf{f} = \bX \bw$, после этого модель зависит только от множества $\calA$, поэтому вместо $(\mathbf{f}, \calA)$ для обозначения используемой модели будем использовать $\calA$. Таким образом, выбор модели сводится к нахождению оптимального множества индексов~$\calA^*$ в смысле функции ошибки $S$, вычисляемой на элементах выборки $\frakD_{\mathcal{C}}$:
%Требуется найти такое подмножество индексов $\mathcal{A} \subset \mathcal{J}$, которое доставляло бы минимум функции $S$ на элементах выборки с индексами из множества $\mathcal{C}$ и вектором параметров $\bw^* \in \mathbb{W}$:
\begin{equation}
\mathcal{A^*} = \mathop{\arg\min}\limits_{\mathcal{A} \subset \mathcal{J}} S(\mathcal{A} | \bw^*, \frakD_{\mathcal{C}}).
\label{eq:featuresel}
\end{equation}

%Запись $f_{\mathcal{A}}$ означает, что для вычисления функции $f$ используются только столбы матриы $\bX$ с индексами из множества $\mathcal{A}$. 
Для решения задачи \eqref{eq:featuresel} необходимо найти вектор параметров $\bw^*$ 
%Требуется найти вектор $\bw^*$ 
как решение задачи минимизации функции ошибки на элементах выборки $\frakD_{\mathcal{L}}$ с индексами из множества $\mathcal{L}$:
\begin{equation}
\bw^* = \mathop{\arg\min}\limits_{\bw \in \mathbb{W}} S(\bw | \frakD_{\mathcal{L}}, \mathcal{A}).
\label{eq:optim_par}
\end{equation} 

Задача $\eqref{eq:featuresel}$ является задачей выбора признаков и заключается в нахождении подмножества индексов признаков $\calA^* \subset \calJ$, минимизирующего функцию ошибки $S$. 
%Таким образом отбирают признаки методы Stepwise и Stagewise.      
%Помимо явного поиска оптимального подмножества признаков (решения задачи~\ref{eq::featuresel}), можно проводить отбор признаков косвенно: обнулением соответствующих элементов вектора параметров $\bw$. Таким методов происходит отбор признаков методами Lars, Lasso, Elastic Net, Ridge.

\section{Анализ мультиколлинеарности при выборе признаков}

В дальнейшем будем считать, что векторы признаков $\bchi_j$ и целевой вектор $\by$ нормированны. Рассмотрим некоторое подмножество $\mathcal{B} \subset \calJ$ индексов признаков.   Назовём признаки \emph{мультиколлинеарными}, если найдутся такие коэффициенты $a_k, \; k \in \mathcal{B}$ и достаточно малое $\delta > 0$, что 
\begin{equation}
\left \| \bchi_j - \sum\limits_{k \in \mathcal{B}} a_k \bchi_k \right \| < \delta,
\label{eq:multicol}
\end{equation}
где $j$~--- индекс признака и $j \not\in \mathcal{B} $. Чем меньше $\delta$, тем выше степень мультиколлинеарности. 

Назовём признаки с индексами $i, j$ \emph{коррелирующими}, если найдётся достаточно малое $\delta_{ij}$ такое, что:
\begin{equation}
\| \bchi_i - \bchi_j \| < \delta_{ij}
\label{eq:correl}
\end{equation}
Из определения следует, что $\delta_{ij} = \delta_{ji}$ и формула \eqref{eq:correl} есть частный случай формулы \eqref{eq:multicol} при $a_k = 0, \; k \neq j$ и $a_k = 1, \; k = j$.

Назовём признак $\bchi_j$ \emph{коррелированным с целевым вектором}, если найдётся достаточно малое $\delta_{yj}$, такое что:
\[
\| \by - \bchi_j \| < \delta_{yj}.
\]
 
%Будем считать, что на множестве параметров $\mathbb{W}$ задано нормальное распределение 
%\[
%\bw \sim \mathcal{N}(\bw_{\mathrm{ML}}, \mathbf{A}^{-1})
%\]   
%с мат. ожиданием $\bw_{\mathrm{ML}}$ и ковариационной матрицей $\mathbf{A}^{-1}$. Оценка $\hat{\bA}^{-1}$ ковариационной матрицы $\bA^{-1}$ в случае линейной модели:
%\[
%\hat{\bA}^{-1} = (\bX^{\T}\bX)^{-1}.
%\]
%
%
%Ковариационная матрица для $\by$, ошибка через сумму....

\subsection{Фактор инфляции дисперсии}
Широко известным критерием анализа мультиколлинеарности авторы считают фактор инфляции дисперсии~\cite{multCausesEffectsRemedies}.  
Фактор инфляции дисперсии $\vif_j$ определяется для $j$-го признака и является показателем наличия линейной зависимости между $j$-ым и остальными признаками. Для нахождения $\vif_j$ необходимо определить оценку $\hat{\bw}$ для вектора коэффициентов $\bw$ в задаче~\eqref{eq:regr} при $y_i = x_{ij}, \; i \in \calI$ и $\calJ = \calJ \setminus j$. Аналогично~\eqref{eq:error} определяются $\rss$ и $\tss$.
Величина $\vif_j$ определяется следующим выражением:
\begin{equation*}
\vif_j = \frac{1}{1 - R^2_j},
\label{eq:vif}
\end{equation*}     
где $R^2_j = 1 - \frac{\rss}{\tss}$~--- коэффициент детерминации. Согласно~\cite{multCausesEffectsRemedies}  значение $\vif_j \gtrsim 5 $ означает наличие зависимости между $j$-ым и всеми остальные признаками.

Недостатками этого критерия мультиколлинеарности является то, что он может принимать большие значения сразу для нескольких признаков, что мешает определить какой из признаков необходимо удалить.   

Другим критерием наличия мультиколлинеарности между признаками является число обусловленности $\kappa$ матрицы $\bX^{\T}\bX$, которое равно отношению максимального и минимального по модулю собственных чисел $\lambda_{\mathrm{max}}$ и $\lambda_{\mathrm{min}}$:
\begin{equation*}
\kappa = \frac{\lambda_{\mathrm{max}}}{\lambda_{\mathrm{min}}}.
\label{eq:condnum}
\end{equation*} 
Оно показывает насколько матрица $\bX^{\T}\bX$ близка к вырожденной. Чем больше $\kappa$, тем ближе матрица к вырожденной.   

\subsection{Метод Белсли}
Для обнаружения и исключения мультиколлинеарных признаков в наборе отобранных признаков предлагается явно поставить оптимизационную задачу, используя метод Белсли. Критерием сравнения методов выбора признаков в данной работе является критерий, основанный на исключении признака, мультиколлинеарного некоторым другим признакам из набора выбранных признаков. Исключение проводится методом Белсли. Формальной записью предлагаемого критерия является задача~\eqref{eq:crit}. Предлагаемый критерий сравнения методов выбора признаков в дальнейшем называется КРИТЕРИЕМ НАЛИЧИЯ МУЛЬТИКОЛЛИНЕАРНЫХ ПРИЗНАКОВ СРЕДИ ОТОБРАННЫХ ПРИЗНАКОВ.  

Будем считать, что на множестве параметров $\mathbb{W}$ задано нормальное распределение 
\[
\bw \sim \mathcal{N}(\bw_{\mathrm{ML}}, \mathbf{A}^{-1})
\]   
с матожиданием $\bw_{\mathrm{ML}}$ и ковариационной матрицей $\mathbf{A}^{-1}$.
Оценка $\hat{\bA}^{-1}$ ковариационной матрицы $\bA^{-1}$ в случае линейной модели:
\[
\hat{\bA}^{-1} = (\bX^{\T}\bX)^{-1}.
\]

Используя сингулярное разложение матрицы $\bX$:
\[
\bX = \bU \boldsymbol{\Lambda} \bV^{\T},
\]
где $\bU$ и $\bV$~--- ортогональные матрицы, а $\boldsymbol{\Lambda}$~--- диагональная с собственными значениями $\lambda_i$ на диагонали, такими что
\[
\lambda_1 > \lambda_2 > \ldots > \lambda_n, 
\] 
получим выражение для ($\bX^{\T}\bX)^{-1}$: 
\begin{equation*}
(\bX^{\T}\bX)^{-1} = \bV\boldsymbol{\Lambda}^{-2}\bV^{-1}.
\end{equation*}
 
Столбцы матрицы $\bV$~--- собственные векторы, а квадраты сингулярных чисел~--- собственные значения матрицы $\bX^{\T}\bX$:
\begin{equation*}
\begin{split}
\bX^{\T}\bX = & \bV\boldsymbol{\Lambda}^{\T}\bU^{\T}\bU
\boldsymbol{\Lambda}\bV^{\T} = \bV\boldsymbol{\Lambda}^2\bV^{\T},\\
&\bX^{\T}\bX\bV = \bV\boldsymbol{\Lambda}^2.
\end{split}
\label{eq:sing_dec}
\end{equation*}
 
Отношение максимального собственного значение $\lambda_{\max}$ к $i$-ому собственному значению $\lambda_i$ назовём индексом обусловленности $\eta_i$
\[
\eta_i = \frac{\lambda_{\max}}{\lambda_i}.
\]
Большое значение $\eta_i$ указывает на зависимость близкую к линейной между признаками и чем больше $\eta_i$ тем сильнее зависимость.
Поэтому на этапе удаления нужно найти такой индекс $i^*$, что
\[
i^* = \mathop{\arg\max}\limits_{i \in \mathcal{F}_{k-1}} \eta_i,
\]
где $\mathcal{F}_{k-1}$ текущее подмножество признаков.

Оценками дисперсий параметров будут диагональные элементы матрицы $\bX^{\T}\bX$:
\[
\text{Var}(w_i) = \sum\limits_{j = 1} ^n \frac{v_{ij}^2} {\lambda^2_j}.
\]
Далее определим дисперсионную долю $q_{ij}$ как вклад $j$-го признака в дисперсию $i$-го элемента вектора параметров $\bw$:
\[
q_{ij} = \frac{v_{ij}^2 / \lambda^2_j}{\sum\limits_{j = 1} ^n v_{ij}^2 / \lambda^2_j},
\] 
где $[v_{ij}] = \bV$, а $\lambda_j$~--- собственное значение. 
Большие значения дисперсионных долей означают наличие зависимостей между признаками, это следует из способа их получения.

Следовательно, по найденному максимальному индексу обусловленности $i^*$ находим признак $j^*$
\begin{equation}
j^* = \mathop{\arg\max}\limits_{j \in \mathcal{F}_{k-1}} q_{i^*j},
\label{eq:belsley}
\end{equation}
который вносит наибольший вклад в дисперсию $i$-го элемента вектора $\bw$, то есть является коллинеарным некоторому другому признаку.

\section{Методы построения тестовых выборок}

Для тестирования методов выбора признаков предлагается использовать следующие синтетические выборки. Определим следующие множества задающие структуру выборки: 
\begin{enumerate}[1)]
\item множество ортогональных признаков $\chi_j$ с индексами $j$ из множества~$\calP_f$;
\item множество признаков $\bchi_j$ ортогональных целевому вектору $\by$ с индексами $j$ из множества~$\calP_y$;
\item множество мультиколлинеарных признаков $\bchi_j$ с индексами $j$ из множества~$\calC_f$;
\item множество признаков $\bchi_j$, коррелирующих с целевым вектором, с индексами $j$ из множества~$\calC_y$ ;
\item множество случайных признаков $\bchi_j$ с индексами из множества~$\calR$.    
\end{enumerate}

Для регулирования степени  мультиколлинеарности используется параметр мультиколлинеарности $k$: при $k = 1$ признаки коллинеарны, при $k = 0$~--- ортогональны. 

При этом параметр $k$ используется как для определения степени мультиколлинеарности признаков, так и для определения степени коррелированности признаков и целевого вектора.

Рассмотрим базовые варианты взаимного расположения мультиколлинеарных признаков и целевого вектора, из которых варьированием параметров можно генерировать различные выборки для тестирования методов выбора признаков.

%\xymatrix{ 
%& &\\
%& \\
%& & & & &\\
%& \\
%*=0{.}\ar[rrrrr]\ar[uuuu]\ar[uurrrr]\ar@[red][uuuu]+/va(45) 10pt/ & & & & &
%}


\begin{enumerate}
\item Признаки $\bchi_j$ с индексами как из множества $ j \in \calC_f$ мультиколлинеарных между собой признаков, так и из множества $ j \in \calP_y$ ортогональных целевому вектору $\by$:
\begin{equation}
\begin{split}
\langle \by, \bchi_j \rangle = 0, \quad j \in \calJ, \quad & 
\left \| \bchi_i - \sum\limits_{l \in \mathcal{B}} \alpha_l\bchi_l \right \| < \delta, \quad i \in \calJ, \quad i \not\in \mathcal{B} \subset \calJ \\
& \calJ = \calP_y \cap \calC_f.
\end{split}
\label{eq:ortcol}
\end{equation}
Схематично взаимное расположение признаков и целевого вектора изображено на рис.~\ref{fig:ortcol}. Выборки с такой структурой будем называть выборками первого типа.  
\begin{figure}[!h]
\begin{equation*}
\xymatrix{
\by & \\
& \\
& \\
*=0{.}\ar@[red][rrrd]_>>{{\large \bchi_1}}+/0 5pt/ %
\ar[uuu] % 
\ar@[red][rrrrd]^>>{{\large \bchi}_2}+/va(-90) 10pt/ % 
\ar@[red][rrrd]+/va(-135) 13pt/ %
\ar@[red][rrd]_{\large{\bchi}_3}!/va(-90) 15pt/ & & & & & \\
& & & \bchi_4 & &
}
\end{equation*}
\caption{}
\label{fig:ortcol}
\end{figure}

%\begin{figure}[!h]
%\centering
%\def\svgwidth{5cm}
%\input{target_ortcol.eps_tex}
%\caption{}
%\label{fig:ortcol}
%\end{figure}

\item Все признаки $\bchi_j$ порождены случайно из многомерной случайной величины. Эта случайная величина взята из равномерного распределения на единичном кубе размерности $r$. Найдётся некоторый признак $\bchi_i$ приближающий целевой вектор $\by$:
\begin{equation}
\calJ = \calR, \quad \bchi_1,\ldots,\bchi_r \sim \mathcal{\mathrm{U}}[0, 1]^r, \quad \|\by - \bchi_i\| < \delta.
\label{eq:random}
\end{equation}

Схематично взаимное расположение признаков и целевого вектора изображено на рис.~\ref{fig:random}. Выборки с такой структурой будем называть выборками второго типа.  

\begin{figure}[!h]
\begin{equation*}
\xymatrix{
& & & \by & & & \\
& & &     & & & \\
& & &     & & & \\
& & & *=0{.}\ar[uuu]_>>{\large \bchi_4} %
\ar@[red][uuu]!/va(70) 80pt/ %
\ar[rrrd] %
\ar[rrr] %
\ar[lld] & & & \bchi_2 \\
& \bchi_3 & & & & & \bchi_1
}
\end{equation*}
\caption{}
\label{fig:random}
\end{figure}
%\begin{figure}[!h]
%\centering
%\def\svgwidth{6cm}
%\input{random.eps_tex}
%\caption{}
%\label{fig:random}
%\end{figure}

\item Все признаки $\bchi_j$ коррелируют и хорошо приближают целевой вектор $\by$:
\begin{equation}
\left \| \bchi_i - \bchi_j \right \| < \delta_{ij}, \quad i, j \in \calJ, \quad \| \by - \bchi_j \| < \delta, \quad j \in \calJ, \quad \calJ = \calC_y 
\label{eq:col}
\end{equation}
Схематично расположение признаков и целевого вектора изображено на рис.~\ref{fig:col}. Выборки с такой структурой будем называть выборками третьего типа.  

\begin{figure}[!h]
\begin{equation*}
\xymatrix{
*=0{.}\ar[rrrddd]^>>{\large \bchi_2} % 
\ar@[red][rrrdd] %
\ar@[red][dddrr]+/va(0) 20pt/ % 
\ar@[red][rrrddd]+/va(50) 15pt/& & & \\
& & & & & & \\
& & & \bchi_3  \\
& & \bchi_1 &  \by 
}
\end{equation*}
\caption{}
\label{fig:col}
\end{figure}

%\begin{figure}[!h]
%\centering
%\def\svgwidth{4cm}
%\input{target_col.eps_tex}
%\caption{}
%\label{fig:col}
%\end{figure}

\item Множество признаков $\bchi_j$ с индексами из множества $j \in \calJ$ состоит из объединения двух множеств: множества ортогональных признаков с индексами из множества $\calP_f$ и множества признаков $\bchi_c$, коррелированных с некоторыми из них. Индексы $c$ лежат в множестве $\calC_f$. При этом целевой вектор $\by$ хорошо приближается линейной комбинацией ортогональных признаков $\bchi_j, \; j \in \calP_f$:
\begin{equation}
\begin{split}
\langle \bchi_i, \bchi_j \rangle = 0, \quad i, j \in \calP_f, \quad & \| \bchi_i - \bchi_j \| < \delta_{ij}, \quad i \in \calP_f, \; j \in \calC_f, \quad \by = \sum\limits_{j \in \calP_f} a_j\bchi_j, \\ 
& \calJ = \calP_f \cup \calC_f.
\end{split}
\label{eq:sumort}
\end{equation}

Схематично взаимное расположение признаков и целевого вектора изображено на рис.~\ref{fig:sumort}. Выборки с такой структурой будем называть выборками четвёртого типа.  

\begin{figure}[!h]
\begin{equation*}
\xymatrix{
& & & \bchi_3 & & & \\
& & &     & & & \\
& & &     & & & \\
& & & *=0{.}\ar[uuu] %
\ar[rrr]_>>{\large \bchi_6}^>>{\large \bchi_7} %
\ar[lldd]_>>{\large \bchi_4}^>>{\large \bchi_5} %
\ar@[red][rrr]!/va(160) 25pt/ %
\ar@[red][rrr]!/va(-90) 13pt/ %
\ar@[red][ddl]+/va(190) 15pt/ %
\ar[rd]
\ar@[red][lld]+/va(-70) 15pt/ & & *={.}\ar@{.}[dl]+/va(140) 7pt/ & \bchi_1 \\
& & *={.}\ar@{.}[rr]!/va(90) 5pt/ & & \by & & \\
& \bchi_2 & & & &  
}
\end{equation*}
\caption{}
\label{fig:sumort}
\end{figure}

%\item Существует некоторое множество ортогональных признаков с индексами $\mathcal{R} \subset \mathcal{J}$. Целевой вектор равен некоторой линейной комбинации признаков с индексами из $\mathcal{R}$. Остальные признаки коллинеарны целевому вектору с ошибкой $\boldsymbol{\varepsilon}_j$. 
%\[
%\bchi_j \perp \bchi_i, \; j \neq i \in \mathcal{R}, \; \by = \bchi_j + \boldsymbol{\varepsilon}_j, \; j \in J \setminus \mathcal{R}, 
%\]
%где $\overline{\| \boldsymbol{\varepsilon}_j \|}^2_2 = d^2$.
%
%Схематичное расположение создаваемых таким образом выборок. Обозначеия аналогичны предыдущему пункту. 
%
%{\centering
%\def\svgwidth{0.7\linewidth}
%\input{target_col.pdf_tex}
%}
\end{enumerate}

Комбинируя вышеописанные варианты взаимного расположения признаков и целевого вектора, варьируя параметр мультиколлинеарности, а также, изменяя мощности $p_f, p_y, c_f c_y$ и $r$ множеств $\calP_f$, $\calP_y$, $\calC_f$, $\calC_y$ и $\calR$, можно генерировать выборки для тестирования методов выбора признаков. 

\section{Критерии сравнения методов выбора признаков}

Для анализа методов выбора признаков определим следующий критерий, позволяющий оценить сколько мультиколлинеарных признаков есть в множестве отобранных признаков. Зададим некоторое предельное значение $s_0$ функции ошибки $S$. Результатом работы метода выбора признаков является набор признаков с индексами из множества $\calA \subset \calJ$. Для найденного множества признаков получен оптимальный вектор параметров $\bw^*_{\calA}$. Назовём $ h $ максимальную мощность множества индексов признаков $\calJ_h \subset \calA $, при удалении которого значение функции ошибки $S$ не превосходит $s_0$:
\begin{equation}
h = \mathop{\arg\max}\limits_{S(\calJ_h, \bw_h, \frakD) \leq s_0} |\calJ_h|,
\label{eq:crit}
\end{equation}
где $S(\calJ_h, \bw_h, \frakD)$~--- функция ошибки, в которой первый аргумент~--- это матрица $\bX$ со столбцами, индексы которых лежат в множестве $\calJ_h$, второй аргумент~--- вектор параметров $\bw_h$, составленный из элементов $\bw^*_{\calA}$ с индексами из множества $\calJ_h$ и третий аргумент~--- выборка. В разделе вычислительный эксперимент определялась величина $d$ равная количеству признаков, удаление которых приводит к ошибке, не превышающей $s_0$:
\[
d = |\calA| - h.
\] 
Определение индексов удаляемых признаков  проводилось методом Белсли, задача~\eqref{eq:belsley}. 
Методы выбора признаков ранжируются по возрастанию величины $d$: большие значения $d$ показывают, что выбранное подмножество признаков $\calA^*$ (решение задачи~\eqref{eq:featuresel}) содержит избыточные признаки, удаление которых приводит к росту функции ошибки вплоть до~$s_0$. 

Ранее авторами в \cite{multCausesEffectsRemedies,Krymova} были предложены следующие критерии сравнения линейных регрессионных моделей:
\begin{enumerate}
%\item Значение функции ошибки $S$ на обучающей выборке, состоящей из элементов выборки $D$ с индексами из множества $\mathcal{L}$ и контрольной выборке, состоящей из элементов выборки $D$ с индексами из множества $\calC$, обозначим соответственно $S(D_{\mathcal{L}} | f_{\calA}, \bw^*)$ и $ S(D_{\mathcal{C}} | f_{\calA}, \bw^*)$. 
\item Скорректированный коэффициент детерминации $\radj$ учитывает добавление избыточных признаков и выражается как:
\[
\radj = 1 - \frac{\rss / (m - k)}{\tss / (m - 1)}.
\]
Чем ближе значение $\radj$ к единице, тем лучше модель описывает целевой вектор.
\item Критерий $C_p$ позволяет достичь компромисса между величиной $\rss$ и количеством используемых переменных $p$, а также ликвидировать возможную коллинеарность признаков. Величина $C_p$ определяется следующим образом:
\[
C_p = \frac{\rss_p}{\rss} - m + 2p,
\]
где $\rss_p$~--- это величина, аналогичная $\rss$, но найденная при использовании $p$ признаков. Меньшие значения $C_p$ соответствуют лучшему набору признаков.    
\item Информационный критерий $\bic$ вычисляется по следующей формуле:
\[
\bic = \rss + p \log m.
\]
Чем меньше величина $\bic$, тем лучше модель описывает целевой вектор.
\item $F$-тест используется в случае линейной модели для проверки отсутствия релевантных признаков. Если ни один из признаков не приближает целевой вектор, то величина 
\[
\frac{(\tss - \rss) / p}{\rss / (n - p - 1)} \sim F_{p, n - p -1}
\]
имеет распределение Фишера с $p, n-p-1$ степенями свободы. 
\end{enumerate}

    

\section{Вычислительный эксперимент}
В вычислительном эксперименте 
Проведено сравнение методов выбора признаков по различным функционалам качества при фиксированном значении предельной функции ошибки $s_0 = 50$ и при двух значениях параметра мультиколлинеарности $k = 0.2$ и $k = 0.8$. Для каждой выборки и для каждого метода выбора признаков были получены зависимости между предельным значением функции ошибки $s_0$ и максимальным числом $d$, а также между фактором инфляции дисперсии $\vif$ и параметром мультиколлинеарности $k$. При этом $\vif$ определялся для признаков из множества $\calA^*$, что показывает наличие мультиколлинеарных признаков в множестве отобранных признаков $\calA^*$.  Эксперименты проводились на выборках при $k = 0.2$ и $k = 0.8$. Для выборок второго типа график зависимости $\vif$ от параметра мультиколлинеарности $k$ и количества избыточных признаков $d$ в множестве отобранных признаков от предельного значения функции ошибки $s_0$ не строился, так как в этом типе выборок нет мультиколлинеарных признаков. 

В экспериментах генерировались выборки четырёх типов, определяемых формулами~\ref{eq:ortcol}, \ref{eq:random}, \ref{eq:col}, \ref{eq:sumort} для двух значений параметра мультиколлинеарности $k = 0.2$ и $k = 0.8$. Перед проведением экспериментов векторы признаков и целевой вектор были отнормированы, так что евклидова норма векторов признаков и целевого вектора равна единице. Измеряемые значения критериев усреднены по 5 повторениям. Значения элементов вектора~$\bw$ меньшие $10^{-6}$ считались незначительными и равными нулю. Значения $p$-value соответствуют проверке нулевой гипотезы о том, что вектор параметров~$\bw$~--- нулевой, против альтернативы, что среди столбцов матрицы~$\bX$ есть подходящие для описания целевого вектора~$\by$, при уровне значимости 0.05. Если значение $p$-value меньше 0.05, то нулевая гипотеза отвергается. Величина предельной функции ошибки $s_0 = 0.5$. 

Сравнивались методы  LARS, Lasso, ElasticNet, Ridge и Stepwise. Все кроме последнего являются методами, которые одновременно решают задачи~\eqref{eq:optim_par} и \eqref{eq:featuresel}. Отбор признаков проводится обнулением незначащих коэффициентов в оптимальном векторе параметров~$\bw^*$. Метод Stepwise последовательно решает задачи~ \eqref{eq:featuresel} и \eqref{eq:optim_par}, добавляя и удаляя признаки в соответствии с их значимостью, определяемой статистическим тестом. Для алгоритма ElasticNet используется одинаковый вес для штрафа Lasso и Ridge равный 0.5. Прочерк в таблице означает, что метод выбора признаков не отбирает ни один признак и получаемый вектор $\bw$ нулевой.

Для выборок первого типа $n = p_y = 50$, $m = 1000$, результаты приведены в таблицах \ref{tab:ortcol_0.2} и \ref{tab:ortcol_0.8} для $k = 0.2$ и $k = 0.8$ соответственно.
\begin{table}[h]
\centering
\caption{Значения функционалов качества для выборок первого типа при $k = 0.2$.}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
 & $d$ & $C_p$ & $\rss$ & $\kappa$ & $\vif$ & $\radj$ & $\bic$ & $p$-value \\ 
\hline 
Lasso & 0  &	$-997$ & 1 & 3.84 & 1.05 & $-3.32$ & 314.62 & 0.11 \\ 
\hline 
Ridge & 0 & $-997$ & 1 & 4.13 & 1.05 & $-3.31$ & 346.39 & 0.1 \\ 
\hline 
LARS & 0 & $-997$ & --- & --- & --- & --- & --- & --- \\ 
\hline 
Stepwise & 0 & $-997$ & 1 & 4.13 & 1.05 & $-3.41$ & 346.41 & $5.28 \cdot 10^{-4}$ \\
\hline
Elastic Net & 0 & $-997$ & 1 & 3.84 & 1.05 & $-3.32$ & 314.32 & 0.11 \\    
\hline 
\end{tabular}
\label{tab:ortcol_0.2}
\end{table}

\begin{table}[h]
\centering
\caption{Значения функционалов качества для выборок первого типа при $k = 0.8$}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
 & $d$ & $C_p$ & $\rss$ & $\kappa$ & $\vif$ & $\radj$ & $\bic$ & $p$-value \\ 
\hline 
Lasso & 0  &	$-997$ & 1 & $717.8$ & 16.6 & $-3.32$ & $310.48$ & 0.06 \\ 
\hline 
Ridge & 0 & $-997$ & 1 & 801 & 16.6 & $-3.31$ & $346.39$ & 0.05 \\ 
\hline 
LARS & --- & $-997$ & --- & --- & --- & --- & --- & --- \\ 
\hline 
Stepwise & 0 & $-997$ & $1.68$ & 801 & 16.6 & $-6.22$ & $347.01$ & $10^{-10}$ \\
\hline
Elastic Net & 0 & $-997$ & 1 & $717.8$ & 16.6 & $-3.32$ & $310.48$ & 0.06 \\
\hline
\end{tabular}
\label{tab:ortcol_0.8}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Для выборок второго типа $n = r = 50$, $m = 1000$, результаты приведены в таблице~\ref{tab:random}.
\begin{table}[h]
\centering
\caption{Значения функционалов качества для выборок второго типа}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
 & $d$ & $C_p$ & $\rss$ & $\kappa$ & $\vif$ & $\radj$ & $\bic$ & $p$-value \\ 
\hline 
Lasso & 0 & $7 \cdot 10^6$ & $8.50 \cdot 10^{-4}$ & 1 & $0.25$ & 1 & $6.9$  & 0 \\ 
\hline
Elastic Net & 0 & $8.76 \cdot 10^{-4}$ & $8.76 \cdot 10^{-4}$ & 1 & $0.25$ & 1 & $ 6.9 $ & 0 \\
\hline
Ridge & 0 & $7.97 \cdot 10^9$ & $0.97$ & 1 & $0.25$ & $-3$ & $ 7.88 $ & 0 \\ 
\hline
LARS & $0.2$ & $-997$ & $1.3 \cdot 10^{-10}$ & $2.19$  & $0.32$ & 1 & $8.29$ & 0 \\ 
\hline 
Stepwise & $4.6$ & $-997$ & $1.33\cdot 10^{-10}$ & $28.86$ & $0.89$ & 1 & $ 53.88$ & 0 \\
\hline  
\end{tabular}
\label{tab:random}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Для выборок третьего типа $n = c_y = 50$, $m = 1000$, результаты приведены в таблицах~\ref{tab:coltarget_0.2} и \ref{tab:coltarget_0.8} для $k = 0.2$ и $k = 0.8$ соответственно.  
\begin{table}[h]
\caption{Значения функционалов качества для выборок третьего типа при $k = 0.2$}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
 & $d$ & $C_p$ & $\rss$ & $\kappa$, $\cdot 10^8$ & $\vif$, $\cdot 10^7$ & $\radj$ & $\bic$ & $p$-value \\ 
\hline
Ridge & 0 & $2.3 \cdot 10^9 $ & $ 0.97 $ & $24$ & $ 1.14$ & $-3.17$ & $346.36$ & 0 \\ 
\hline 
Lasso & 1  & $2\cdot 10^6$ & $8.5 \cdot 10^{-4}$  & $0.95$ & $0.58$ & 1 & $13.82$ & 0 \\ 
\hline
Elastic Net & $3.2$ & $2 \cdot 10^6$ & $8.5 \cdot 10^{-4}$  & $2.8$ & $0.97$ & 1 & $41.45$  & 0 \\ 
\hline 
Stepwise & 36 & $-997$ & $4.22 \cdot 10^{-10}$  & $24$  & $ 1.14$ & 1  & $345.39$ & 0 \\
\hline 
LARS & 36 & $-997$ & $4.22 \cdot 10^{-10}$ & $24$ & $1.14$ & 1 & $345.39$ & 0 \\ 
\hline 
\end{tabular}
\label{tab:coltarget_0.2}
\end{table}

\begin{table}[h]
\centering
\caption{Значения функционалов качества для выборок третьего типа при $k = 0.8$}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
 & $d$ & $C_p$ & $\rss$ & $\kappa$ & $\vif$ & $\radj$ & $\bic$ & $p$-value \\ 
\hline 
Lasso & 0 & $5.16 \cdot 10^8$ & $8.5 \cdot 10^{-4}$ & 1 & $0.24$ & 1 & $6.9$ & 0 \\ 
\hline
Ridge & 0 & $5.9 \cdot 10^{11}$ & $0.97$ & $6.07 \cdot 10^{11}$ & $2.9 \cdot 10^9$ & $-3.17$ & $346.36$  & 0\\  
\hline 
Elastic Net & $3.2$ & $5.16 \cdot 10^8$ & $8.5 \cdot 10^{-4}$ & $7.3 \cdot 10^{10}$ & $2.5 \cdot 10^9$ & 1  & $41.45$ & 0 \\ 
\hline  
Stepwise & 36 & $-997$ & $1.73 \cdot 10^{-12}$ & $ 6.07 \cdot 10^{11}$ & $ 2.9 \cdot 10^9 $ & 1 & $345.39$ & 0 \\
\hline
LARS & 36 & $-997$ & $ 1.65 \cdot 10^{-12} $ & $6.07 \cdot 10^11$ & $ 2.9 \cdot 10^9$ & 1 & $ 345.39$ & 0\\
\hline
\end{tabular}
\label{tab:coltarget_0.8}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Для выборок четвёртого типа $p_f = 10$, $ c_f = 40$, $m = 1000$, результаты приведены в таблицах~\ref{tab:sumort_0.2} и \ref{tab:sumort_0.8} для $k = 0.2$ и $k = 0.8$ соответственно.
\begin{table}[!h]
\centering
\caption{Значения функционалов качества для выборок четвёртого типа при $k = 0.2$}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
 & $d$ & $C_p$ & $\rss$ & $\kappa$ & $\vif$ & $\radj$ & $\bic$ & $p$-value \\ 
\hline 
Ridge & 0 & $6 \cdot 10^{30}$ & $0.95$ & $8.42 \cdot 10^{15}$ & $ 1.15 \cdot 10^{23} $ & $-3$ & $210.95$ & 0 \\ 
\hline 
Stepwise & 1 & $-868.95$ & $ 5.45 \cdot 10^{-29} $ & 1 & $0.63$ & 1 & $13.82$ & 0 \\
\hline
LARS & $1.8$ & $ 5.38 \cdot 10^{29} $ & $ 0.38$ & $ 2.1 \cdot 10^{16}$ & $3.3 \cdot 10^{30}$ & $-0.62$ & $102.62$  & 0 \\ 
\hline 
Lasso & 18 & $5.84 \cdot 10^{27}$ & $ 9.18 \cdot 10^{-4}$ & $1.4 \cdot 10^{16}$ & $ 5.32 \cdot 10^{20}$ & 1  & $150.6$ & 0 \\ 
\hline 
Elastic Net & $17.6$ & $5.84 \cdot 10^{27}$ & $ 9.18 \cdot 10^{-4}$ & $1.4 \cdot 10^{16}$ & $ 5.32 \cdot 10^{20} $ & 1 & $150.59$ & 0 \\
\hline
\end{tabular}
\label{tab:sumort_0.2}
\end{table} 

\begin{table}[!h]
\centering
\caption{Значения функционалов качества для выборок четвёртого типа при $k = 0.8$}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
 & $d$ & $C_p$ & $\rss$ & $\kappa$ & $\vif$ & $\radj$ & $\bic$ & $p$-value \\ 
\hline 
Ridge & 0 & $1.8 \cdot 10^{30}$ & $0.95$ & $10^{16}$ & $8.65 \cdot 10^{16}$ & $-2.97$ & $152.92$ & 0 \\ 
\hline
Stepwise & 1 & $9.4 \cdot 10^5$ & $8.8 \cdot 10^{-25}$ & 1 & $0.63$ & 1 & $13.82$ & 0 \\
\hline  
LARS & $1.2$ & $10^{30}$ & $0.38$ & $3 \cdot 10^{29}$ & $10^{20}$ & $-0.57$ & $108.15$ & 0 \\ 
\hline
Lasso & $ 14.8 $ & $ 1.73 \cdot 10^{27}$ & $ 9.2 \cdot 10^{-4}$ & $ 9.92 \cdot 10^{15}$ & $ 10^{17}$ & 1 & $150.59$ & 0 \\ 
\hline 
Elastic Net & $15.2$ & $1.7 \cdot 10^{27}$ & $9.2 \cdot 10^{-4}$ & $9.92 \cdot 10^{15}$ & $10^{17}$ & 1 & $150.59$ & 0 \\
\hline
\end{tabular}
\label{tab:sumort_0.8}
\end{table} 

На рисунках~\ref{fig:Vif_k_ortcol}, \ref{fig:Vif_k_coltarget}, \ref{fig:Vif_k_sumort} представлена зависимость $\vif$ от параметра мультиколлинеарности $k$ для каждого типа выборок, где эта зависимость имеет место. 

На рис.~\ref{fig:Vif_k_ortcol} показана зависимость $\vif$ от параметра мультиколлинеарности $k$ для первого типа выборок при работе различных алгоритмов. На рисунке видно, что все алгоритмы показывают одинаковые результаты, и ни один из рассматриваемых методов выбора признаков не решает проблему мультиколлинеарности в случае ортогональности всех признаков целевому вектору и взаимной коррелированности.

\begin{figure}[!h]
\centering
\includegraphics[scale = 0.6]{VIF_k_ortcol_all.eps}
\caption{Зависимость величины фактора инфляции дисперсии $\vif$ от параметра мультиколлинеарности $k$ для первого типа выборок}
\label{fig:Vif_k_ortcol}
\end{figure} 

На рис.~\ref{fig:Vif_k_coltarget} изображена зависимость $\vif$ от параметра мультиколлинеарности $k$ для третьего типа выборок. Видно, что все методы показывают одинаковый вид зависимости, кроме метода Lasso. Для него при росте параметра мультиколлинеарности наблюдается резкое уменьшение величины $\vif$. Это говорит об отсутствии линейной зависимости между выбранными признаками в выборках, сгенерированных при $k \gtrsim 0.4$.

\begin{figure}[h]
\centering
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{VIF_k_coltarget_all2.eps}
\label{fig:Vif_k_coltarget_all}
}
~
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{VIF_k_coltarget_Lasso2.eps}
\label{fig:Vif_k_coltarget_Lasso}
}
\caption{Зависимость фактора инфляции дисперсии $\vif$ от параметра мультиколлинеарности $k$ для третьего типа выборок при работе:~\subref{fig:Vif_k_coltarget_all} всех рассматриваемых методов отбора признаков, \subref{fig:Vif_k_coltarget_Lasso} только Lasso}
\label{fig:Vif_k_coltarget}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

На рис.~\ref{fig:Vif_k_sumort} показана зависимость $\vif$ от параметра мультиколлинеарности $k$ для четвёртого типа выборок при работе различных методов. Метод LARS показывает резкие скачки значений $\vif$ (рис.~\ref{fig:Vif_k_sumort} \subref{fig:Vif_k_sumort_all}). Это не позволяет оценить зависимость $\vif$ от $k$ для других методов, поэтому на рис.~\ref{fig:Vif_k_sumort} \subref{fig:Vif_k_sumort_all_lars} изображены аналогичные графики для всех рассматриваемых методов, кроме LARS. Методы Lasso и ElasticNet демонстрируют скачки, схожие со скачками у LARS, но меньшей амплитуды. Поэтому зависимости $\vif$ от $k$, получаемые после использования методов Stepwise и Ridge, изображены на рис.~\ref{fig:Vif_k_sumort}  \subref{fig:Vif_k_sumort_Stepwise} и~\ref{fig:Vif_k_sumort} \subref{fig:Vif_k_sumort_Ridge}. Для выборок четвёртого типа после применения метода Stepwise значения $\vif$ не превышают двух при росте коэффициента $k$. Это означает, что метод Stepwise для выборок четвёртого типа даёт набор признаков, в котором нет линейной зависимости между признаками.   

\begin{figure}[!h]
\centering
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{VIF_k_sumort_all.eps}
\label{fig:Vif_k_sumort_all}
}
~
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{VIF_k_sumort_all_lars.eps}
\label{fig:Vif_k_sumort_all_lars}
}
\\
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{VIF_k_sumort_Stepwise.eps}
\label{fig:Vif_k_sumort_Stepwise}
}
~
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{VIF_k_sumort_Ridge.eps}
\label{fig:Vif_k_sumort_Ridge}
}
\caption{Зависимость величины фактора инфляции дисперсии $\vif$ от параметра мультиколлинеарности $k$ для четвёртого типа выборок при работе:~\subref{fig:Vif_k_sumort_all}~всех методов отбора признаков, \subref{fig:Vif_k_sumort_all_lars} всех методов кроме LARS, \subref{fig:Vif_k_sumort_Stepwise} метода Stepwise, \subref{fig:Vif_k_sumort_Ridge} метода Ridge }
\label{fig:Vif_k_sumort}
\end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Рассмотрим зависимость количества мультиколлинеарных признаков $d$ в множестве отобранных признаков от значений предельной ошибки $s_0$ для ранее рассмотренных типов выборок на рис.~\ref{fig:d_s0_ortcol}, \ref{fig:d_s0_coltarget}, \ref{fig:d_s0_sumort}. 

На рис.~\ref{fig:d_s0_ortcol} показана зависимость количества лишних признаков $d$ в множестве отобранных признаков от предельного значения функции ошибки $s_0$ для первого типа выборок при значениях $k = 0.2$ и $k = 0.8$. Величина $d$ стабильно равна нулю из-за ортогональности целевого вектора и всех признаков вплоть до значений $s_0$ близкими к единице. Далее идёт резкий скачок $d$, так как предельное значение функции ошибки выросло достаточно, чтобы удалить сразу почти все признаки.  

\begin{figure}[!h]
\centering
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{d_s0_ortcol_02_all.eps}
\label{fig:d_s0_ortcol_02_all}
}
~
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{d_s0_ortcol_08_all.eps}
\label{fig:d_s0_ortcol_08_all}
}
\caption{Зависимость количества мультиколлинеарных признаков $d$ в множестве отобранных признаков от предельного значения функции ошибки $s_0$ для первого типа выборок при:~\subref{fig:d_s0_ortcol_02_all} $k = 0.2$, \subref{fig:d_s0_ortcol_08_all} $k = 0.8$}
\label{fig:d_s0_ortcol}
\end{figure}

На рис.~\ref{fig:d_s0_coltarget} показана зависимость величины $d$ от параметра $s_0$ для третьего типа выборок при значении $k = 0.2$ и $k = 0.8$. Метод Lasso отбирает один или два признака, наилучшим образом приближающие целевой вектор, поэтому величина $d$ для этого метода равна нулю или единице. Аналогично, но чуть хуже работает метод ElasticNet, он отбирает чуть больше лишних признаков нежели метод Lasso. Зависимость $d$ от $s_0$ для метода Ridge схожа с зависимостью для первого типа выборок по той же причине: сначала $s_0$ достаточна велика, чтобы удалять хоть одни признак, но как только $s_0$ приближается к единице становится возможным удалить сразу почти все признаки. Для методов LARS и Sterwise наблюдается постепенный рост величины $d$ с ростом предельного значения функции ошибки $s_0$ с выходом на константу при достижении $s_0$ значения близкого к единице.   

\begin{figure}[!h]
\centering
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{d_s0_coltarget_02_all.eps}
\label{fig:d_s0_coltarget_02_all}
}
~
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{d_s0_coltarget_08_all.eps}
\label{fig:d_s0_coltarget_08_all}
}
\caption{Зависимость количества мультиколлинеарных признаков $d$ в множестве отобранных признаков от предельного значения функции ошибки $s_0$ для третьего типа выборок при:~\subref{fig:d_s0_coltarget_02_all} $k = 0.2$, \subref{fig:d_s0_coltarget_08_all} $k = 0.8$}
\label{fig:d_s0_coltarget}
\end{figure}


На рис.~\ref{fig:d_s0_sumort} показана зависимость $d$ от параметра $s_0$ для четвёртого типа выборок при $k = 0.2$ и $k = 0.8$. Наиболее стабильные решения даёт метод Stepwise, у которого в среднем обнаруживается только один признак, удаление которого приводит к ошибке, не превышающей $s_0$. Чуть хуже работает метод LARS: количество лишних признаков $d$ среди отобранных им не превышает пяти при росте предельного значения функции ошибки $s_0$. Для методов Lasso и ElasticNet наблюдается рост $d$ при росте $s_0$ до единице, а затем стабилизация на уровне $d \simeq 20$. Для метода Ridge вид зависимости схож с предыдущими типами выборок, только для четвёртого типа после преодоления $s_0$ значения равного единице величина $d$ начала сильно колебаться. Это показывает неустойчивость набора признаков, получаемого методом Ridge для четвёртого типа выборок. 

\begin{figure}[!h]
\centering
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{d_s0_sumort_02_all.eps}
\label{fig:d_s0_sumort_02_all}
}
~
\subfigure[]{
\includegraphics[width = 0.47\textwidth]{d_s0_sumort_08_all.eps}
\label{fig:d_s0_sumort_08_all}
}
\caption{Зависимость количества мультиколлинеарных признаков $d$ в множестве отобранных признаков от предельного значения функции ошибки $s_0$ для четвёртого типа выборок при:~\subref{fig:d_s0_sumort_02_all} $k = 0.2$, \subref{fig:d_s0_sumort_08_all} $k = 0.8$}
\label{fig:d_s0_sumort}
\end{figure}


\section{Заключение}
В работе проведено исследование эффективности методов выбора признаков в случае выборок с мультиколлинеарными признаками. Эксперименты показали, что из рассмотренных методов проблему мультиколлинеарности при отборе признаков решают методы Lassо (для выборок третьего типа) и Stepwise (для выборок четвёртого типа). Для выборок первого типа все рассмотренные методы показывают одинаковые результаты: ни один из рассматриваемых методов выбора признаков не решает проблему мультиколлинеарности в случае ортогональности всех признаков целевому вектору. Предложенный критерий показывает, что как при малых, так и при больших значениях $k$ устойчивые решения дают одинаковые методы. Также вид зависимости между величинами $s_0$ и $d$ практически одинаков в рамках одной выборки для больших и маленьких значений  $k$. Для выборок первого типа все рассматриваемые методы показывают одинаковый результат, для выборок третьего типа наиболее устойчивый результат даёт метод Lasso, для выборок четвёртого типа~--- методы LARS и Stepwise.
%format.volume output
%\printbibliography 
%\bibliographystyle{ugost2008}
%\bibliographystyle{gost}
%\bibliography{lib_ru}
\renewcommand\refname{Литература}


\input{lib_rus.bbl}
\renewcommand\refname{References}
\input{lib_eng.bbl}

\end{document}